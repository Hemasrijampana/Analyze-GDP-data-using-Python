# -*- coding: utf-8 -*-
"""GDP Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAwTJw7EwgEFaWTnfvX59ErDYuT1ZaGg

**Upload the file:**

This allows you to upload files from your local machine to your Google Colab environment for further processing or analysis within your notebook.
"""

from google.colab import files

uploaded = files.upload()

"""# Load Data from CSV File :

It loads data from a CSV file into a Pandas DataFrame and then displays the contents of that DataFrame.
"""

import numpy as np
import pandas as pd
data1=pd.read_csv("countries of the world (1).csv")
data1

df=pd.DataFrame(data1 )
df

"""**dropna():**

In Pandas, dropna() is used to remove rows with missing values (NaN/null values) from a DataFrame. By default, it removes rows where any of the columns have NaN values. If you want to remove columns with NaN values instead, you would specify axis=1. The function returns a new DataFrame with the NaN values dropped.
"""

df1= df.dropna()
df1

"""**df1.isnull().sum() :**

returns a Series (with column names as index) where each element represents the count of null values in the corresponding column of DataFrame df1. It's a quick way to get a summary of missing values in each column of a DataFrame.
"""

df1.isnull().sum()

"""**.isnull() :**
is a method in Pandas used to check for missing or null values within a DataFrame. 'True' indicates that the corresponding element in dataframe is a null value, and 'False' indicates that it's not a null value.
"""

df1.isnull()

"""**describe() :** is a method in Pandas used to generate descriptive statistics of the DataFrame."""

df1.describe()

df1.isnull().sum()

"""**dtypes :** is an attribute in Pandas that returns the data type of each column in the DataFrame"""

df1.dtypes

"""**.drop():** This is a method in Pandas used to remove rows or columns from a DataFrame.

**df.columns[-1]:** This part accesses the last column name from the DataFrame df. df.columns returns a list-like object containing column names, and [-1] index retrieves the last column name.

**axis=1:** This specifies that we are dropping a column (as opposed to a row). In Pandas, axis=0 refers to rows and axis=1 refers to columns.

Here, we're dropping **GDP** column to calculate it using **GDP_Per_Capita**
and **Population**.
"""

df1 = df1.drop(df.columns[-1], axis=1)

"""**.head() :** head() method in Pandas is used to view the first few rows of a DataFrame. By default, it displays the first 5 rows."""

df1.head()

df1.dtypes

"""# **Calculating GDP:**
calculates the GDP (Gross Domestic Product) by multiplying **GDP per capita** with the **population** for each row in the DataFrame.
"""

df1['gdp'] = df1['GDP_per_capita'] * df1['Population']
df1.head()

""".apply(lambda x: "{:.2f}".format(x)):

**.apply() :** function in Pandas is used to apply a function along an axis of the DataFrame.

**lambda x: "{:.2f}".format(x):** This lambda function takes a value x and formats it as a string with two decimal places ("{:.2f}").
"""

df1['gdp'] = df1['gdp'].apply(lambda x: "{:.2f}".format(x))
df1.head()

"""# MinMaxScaler

**MinMaxScaler class** from the **sklearn.preprocessing module** to perform feature scaling on selected columns of a DataFrame.

The **MinMaxScaler** is a **preprocessing algorithm** used to scale numerical features to a given range, typically between 0 and 1.

**scaler = MinMaxScaler():** This creates an instance of the MinMaxScaler class, which will be used to scale the features.
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df1[['Population', 'GDP_per_capita','gdp']] = scaler.fit_transform(df1[['Population', 'GDP_per_capita','gdp']])
df1.head()

"""# Different Graphs and Plots :
visualizes the distribution of a numerical variable across different categories

**Violin Plot :**

A violin plot is a method of plotting numeric data and can be considered a combination of a box plot and a kernel density plot. It displays the distribution of the data by showing its probability density along the y-axis.

**.violinplot(x='Region', y='GDP_per_capita', data=df1):**

This line creates a violin plot using Seaborn. It specifies that the variable 'GDP_per_capita' should be plotted on the y-axis, and the variable 'Region' should be used to group the data on the x-axis. The data for plotting is provided by the DataFrame
"""

import seaborn as sns
import matplotlib.pyplot as plt
# Adjust the figure size as needed
plt.figure(figsize=(12, 8))
sns.violinplot(x='Region', y='GDP_per_capita', data=df1)
plt.xticks(rotation=45)
plt.show()

"""# Scatter Matrix Plot :

A scatter matrix plot is a grid of scatter plots where each cell in the grid represents the relationship between two variables. It's a useful tool for visualizing pairwise relationships between multiple variables in a dataset.

**import plotly.express as px:**

This line imports the Plotly Express library, which is a high-level interface for creating interactive plots with Plotly.

**px.scatter_matrix(df1, dimensions=['Population', 'GDP_per_capita']):**

This line creates a scatter matrix plot using Plotly Express. The df1 DataFrame is passed as the data source for plotting, and the dimensions parameter specifies which variables from the DataFrame should be plotted. In this case, the scatter matrix will display relationships between the 'Population' and 'GDP_per_capita' variables.
"""

import plotly.express as px
fig = px.scatter_matrix(df1, dimensions=['Population',  'GDP_per_capita'])
fig.show()

"""# Choropleth Map :
A choropleth map is a thematic map in which areas are shaded or patterned in proportion to the value of a variable being represented. It's commonly used to visualize spatial data, such as population density, GDP per capita, or election results, across geographical regions.


"""

import plotly.express as px
fig = px.choropleth(df1,
                    locations='Country',
                    locationmode='country names',
                    color='GDP_per_capita',
                    hover_name='Country',
                    color_continuous_scale='Viridis',
                    title='GDP per Capita Choropleth Map')

fig.update_geos(showcoastlines=True, coastlinecolor="black", showland=True, landcolor="white")

fig.show()

#import pandas as pd
#categorical_columns = df1.select_dtypes(include='category').columns

# Convert categorical columns to codes
#df1[categorical_columns] = df1[categorical_columns].apply(lambda x: x.cat.codes)
#print("DataFrame after converting categorical columns to codes:")
#print(df1)

df1.dtypes

df1.isnull().sum()

"""#Categorical Datatype :
Categorical data type is useful when dealing with columns with a limited and known set of unique values (categories). It can save memory and speed up certain operations, especially when performing groupings and aggregations.

**.astype():**
The .astype() method is used to change the data type of a Series (column) in Pandas. By specifying 'category' as the argument, it instructs Pandas to treat the 'Country' column as a categorical variable
"""

df1['Country'] = df1['Country'].astype('category')
df1['Region'] = df1['Region'].astype('category')

df1.columns

df1['gdp'].dtype

for column in df1.columns:
    if df1[column].dtype == 'object':
      df1[column] = df1[column].astype('category')

df1.dtypes

import pandas as pd
categorical_columns = df1.select_dtypes(include='category').columns

# Convert categorical columns to codes
df1[categorical_columns] = df1[categorical_columns].apply(lambda x: x.cat.codes)
print("DataFrame after converting categorical columns to codes:")
print(df1)

"""**.corr() :** calculates the correlation matrix for the DataFrame

he correlation matrix is a square matrix where the entries on the diagonal are always 1 (since each variable perfectly correlates with itself), and the off-diagonal entries represent the correlation coefficient between pairs of variables. The correlation coefficient measures the strength and direction of the linear relationship between two variables. It ranges from -1 to 1, where:
*   1 indicates a perfect positive correlation (as one variable increases, the other variable also increases linearly).
*   -1 indicates a perfect negative correlation (as one variable increases, the other variable decreases linearly).
*   0 indicates no linear correlation between the variables.





"""

cor1  =df1.corr()
cor1

"""#Heat Map

A heatmap is a graphical representation of data where the individual values contained in a matrix are represented as colors. In this case, the heatmap displays the correlation matrix, with each cell color representing the strength and direction of the correlation between pairs of variables. Positive correlations are typically represented by lighter colors, while negative correlations are represented by darker colors.

The **annot=True** parameter adds numerical annotations to each cell of the heatmap, displaying the correlation coefficients.
"""

import seaborn as sns
plt.figure(figsize=[20,20])
sns.heatmap(cor1,annot=True)

"""#unstack() :
The unstack() method reshapes a multi-level index DataFrame (in this case, the correlation matrix cor1) into a one-dimensional Series. This operation effectively lists all the pairwise correlations between variables, with each entry representing the correlation between two variables.

**[:50] :**  This slice notation is used to select the top 50 entries from the sorted Series. It retrieves the first 50 elements of the Series, which correspond to the pairs of variables with the highest correlations.
"""

cor_pairs = cor1.unstack()
cor_pairs.sort_values(ascending=False)[:50]

"""#Regression Plot :
A regression plot (regplot)  visualize the relationship between two variables: the target variable ('gdp') and the feature ('Area (sq. mi.)').

Here, regression plot visualizes the relationship between the 'Area (sq. mi.)' feature and the 'gdp' target variable, providing insights into how changes in the area might impact GDP per capita.
"""

import seaborn as sns
import matplotlib.pyplot as plt
target_variable = 'gdp'
feature = 'Area (sq. mi.)'
plt.figure(figsize=(10, 6))
sns.regplot(x=feature, y=target_variable, data=df1)
plt.xlabel(f'{feature} (%)')
plt.ylabel(f'{target_variable} per Capita')
plt.title(f'Regplot: {target_variable} vs {feature}')
plt.show()

for column in df1.columns:
  if df1[column].dtype=='category':
    df1[column] = df1[column].cat.codes

"""#PairPlot :

A pairplot that visualizes the relationships between the 'gdp' target variable and the specified feature variables ('Service', 'Net migration', 'Literacy (%)'). Each subplot in the pairplot represents a scatter plot with a regression line, showing the relationship between the target variable and each feature variable individually. This visualization helps in understanding how changes in the feature variables may impact the target variable.
"""

import seaborn as sns
import matplotlib.pyplot as plt
target_variable = 'gdp'
feature_variables = ['Service', 'Net migration', 'Literacy (%)']
sns.pairplot(df1, x_vars=feature_variables, y_vars=target_variable, kind='reg', height=5)

plt.suptitle(f'Scatter Plot with Regression Line: {target_variable} vs Features', y=1.02)
plt.show()

cor2  =df1.corr()
cor2

import seaborn as sns
plt.figure(figsize=[14,14])
sns.heatmap(cor2,annot=True)

cor_pairs = cor2.unstack()
cor_pairs.sort_values(ascending=False)[:60]

"""#Pie Chart :
The pie chart that visualizes the distribution of GDP per capita for the top 5 countries with the highest mean GDP per capita in the dataset. Each wedge of the pie chart represents the proportion of GDP per capita contributed by each of the top 5 countries.






"""

import pandas as pd
import matplotlib.pyplot as plt
top_countries = df.groupby('Country')['GDP_per_capita'].mean().nlargest(5)
plt.figure(figsize=(8, 8))
plt.pie(top_countries, labels=top_countries.index, autopct='%1.1f%%', startangle=90)
plt.title('GDP_per_capita Distribution for Top 5 Countries')
plt.show()

"""#Linear Regression Algorithm :"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
features = df1[['Population', 'Area (sq. mi.)', 'Pop. Density (per sq. mi.)', 'Coastline (coast/area ratio)',
               'Net migration', 'Infant mortality (per 1000 births)', 'Literacy (%)', 'Arable (%)',
               'Crops (%)', 'Other (%)', 'Climate', 'Birthrate', 'Deathrate', 'Agriculture', 'Industry', 'Service']]

target = df1['GDP_per_capita']

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model = LinearRegression()

model.fit(X_train_scaled, y_train)
predictions = model.predict(X_test_scaled)

mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

df2=df1.drop(['Population','Area (sq. mi.)','Pop. Density (per sq. mi.)','Infant mortality (per 1000 births)','Arable (%)','Crops (%)','Other (%)','Birthrate', 'Deathrate', 'Agriculture', 'Industry','gdp'],axis=1)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
features = df2[['Coastline (coast/area ratio)','Net migration', 'Literacy (%)','Phones (per 1000)','Climate','Service']]

target = df2['GDP_per_capita']


X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model3 = LinearRegression()

model3.fit(X_train_scaled, y_train)
predictions = model3.predict(X_test_scaled)

mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

import pandas as pd
Coastline= int(input("Enter the Coastline (coast/area ratio): "))
Net = int(input("Enter the Net migration: "))
Literacy = float(input("Enter the Literacy (%): "))
Phones = float(input("Enter the Phones (per 1000): "))
Climate = float(input("Enter the Climate: "))
Service = float(input("Enter the Service: "))
GDP_= float(input("Enter the GDP: "))

new_data = {"Coastline (coast/area ratio)":Coastline, "Net migration": Net,"Literacy (%)": Literacy, "Phones (per 1000)": Phones , "Climate": Climate, "Service": Service,"GDP_per_capita": GDP_}
new_df = pd.DataFrame([new_data])
new_df_features = new_df.drop('GDP_per_capita', axis=1)
new_df_features = pd.get_dummies(new_df_features)
predicted_value = model3.predict(new_df_features)[0]
print(f"Predicted GDP_Per_Capita : {predicted_value}")

"""# Decision Tree Regression :

Decision Tree is a supervised learning algorithm used for both classification and regression tasks. In Decision Tree Regression, the algorithm splits the data into subsets based on the value of input features to predict the target variable. It builds a tree-like structure where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents the predicted value. The goal is to minimize the impurity or variance in each split. Decision trees are easy to interpret and understand, making them popular for predictive modeling tasks.
"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Define features and target
features = df2[['Coastline (coast/area ratio)', 'Net migration', 'Literacy (%)', 'Phones (per 1000)', 'Climate', 'Service']]
target = df2['GDP_per_capita']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train decision tree model
model_decision_tree = DecisionTreeRegressor(random_state=42)
model_decision_tree.fit(X_train_scaled, y_train)
predictions_decision_tree = model_decision_tree.predict(X_test_scaled)

# Evaluate model
mse = mean_squared_error(y_test, predictions_decision_tree)
r2 = r2_score(y_test, predictions_decision_tree)
print("Decision Tree:")
print(f"\tMean Squared Error: {mse}")
print(f"\tR-squared: {r2}")

"""# K-Nearest Neighbors Algorithm:

The K-Nearest Neighbors (KNN) algorithm is a simple and intuitive supervised learning algorithm used for both classification and regression tasks. In KNN regression, the predicted value for a new data point is calculated by averaging the target values of the k-nearest data points in the training set.
"""

from sklearn.neighbors import KNeighborsRegressor
# (Same imports and data preprocessing as in the previous code snippet)

# Train k-nearest neighbors model
model_knn = KNeighborsRegressor()
model_knn.fit(X_train_scaled, y_train)
predictions_knn = model_knn.predict(X_test_scaled)

# Evaluate model
mse = mean_squared_error(y_test, predictions_knn)
r2 = r2_score(y_test, predictions_knn)
print("K-Nearest Neighbors:")
print(f"\tMean Squared Error: {mse}")
print(f"\tR-squared: {r2}")

"""# Support Vector Machine Algorithm:

Support Vector Machine (SVM) is a powerful supervised learning algorithm used for both classification and regression tasks. In the case of regression, it is referred to as Support Vector Regression (SVR).
"""

from sklearn.svm import SVR
# (Same imports and data preprocessing as in the previous code snippet)

# Train support vector machine model
model_svm = SVR()
model_svm.fit(X_train_scaled, y_train)
predictions_svm = model_svm.predict(X_test_scaled)

# Evaluate model
mse = mean_squared_error(y_test, predictions_svm)
r2 = r2_score(y_test, predictions_svm)
print("Support Vector Machine:")
print(f"\tMean Squared Error: {mse}")
print(f"\tR-squared: {r2}")

"""# Random Forest Algorithm :

Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the average prediction of the individual trees.

**Bagging (Bootstrap Aggregating):** Random Forest uses a technique called bagging to build multiple decision trees from random subsets of the training data with replacement. Each subset is called a bootstrap sample. Bagging helps to reduce overfitting by introducing diversity among the trees.

**Random Feature Selection:** At each node of the decision tree, Random Forest randomly selects a subset of features to consider for splitting. This helps to decorrelate the trees and further reduce overfitting.

**Voting or Averaging:** For regression tasks (like in this code snippet), Random Forest averages the predictions of individual trees to make the final prediction. For classification tasks, it uses voting to determine the class with the most votes among the individual trees.
"""

from sklearn.ensemble import RandomForestRegressor
# (Same imports and data preprocessing as in the previous code snippet)

# Train random forest model
model_random_forest = RandomForestRegressor(random_state=42)
model_random_forest.fit(X_train_scaled, y_train)
predictions_random_forest = model_random_forest.predict(X_test_scaled)

# Evaluate model
mse = mean_squared_error(y_test, predictions_random_forest)
r2 = r2_score(y_test, predictions_random_forest)
print("Random Forest:")
print(f"\tMean Squared Error: {mse}")
print(f"\tR-squared: {r2}")

import pandas as pd

# Collect input from the user
Coastline = int(input("Enter the Coastline (coast/area ratio): "))
Net = int(input("Enter the Net migration: "))
Literacy = float(input("Enter the Literacy (%): "))
Phones = float(input("Enter the Phones (per 1000): "))
Climate = float(input("Enter the Climate: "))
Service = float(input("Enter the Service: "))

# Create a new DataFrame with user input
new_data = {
    "Coastline (coast/area ratio)": [Coastline],
    "Net migration": [Net],
    "Literacy (%)": [Literacy],
    "Phones (per 1000)": [Phones],
    "Climate": [Climate],
    "Service": [Service]
}
new_df = pd.DataFrame(new_data)

# Preprocess the new DataFrame (similar to how X_train_scaled was prepared)
new_df_features = scaler.transform(new_df)

# Predict GDP per capita using the trained Random Forest model
predicted_value = model_random_forest.predict(new_df_features)[0]

# Print the predicted GDP per capita
print(f"Predicted GDP_Per_Capita: {predicted_value}")

"""# **Conclusion :**

We can conclude that the "Random Forest algorithm" achieved the **lowest Mean Squared Error (MSE)** and the **highest R-squared value**, indicating better predictive performance compared to the other algorithms.

Therefore, based on this evaluation, it can be concluded that the **Random Forest algorithm** is more suitable for predicting the target variable "GDP_Per_Capita" and provides higher accuracy compared to Linear Regression, KNN, and SVM algorithms.
"""